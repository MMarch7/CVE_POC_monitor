#!/usr/bin/env python 
# -*- coding: utf-8 -*-
# @Time    : 2025/03/04
# @Author  : LXY
# @File    : main.py
# @Github: https://github.com/MMarch7
import datetime
import feedparser
import logging
import os
#import dingtalkchatbot.chatbot as cb
import requests
import re
import utils.load
from urllib.parse import quote
import msg_push
import csv

github_token = os.environ.get("github_token")
repo_list,keywords,user_list = utils.load.load_tools_list()
CleanKeywords = utils.load.load_clean_list()
known_object = utils.load.load_object_list()
github_sha = "./utils/sha.txt"

def load_processed_shas():
    if not os.path.exists(github_sha):
        return set()
    with open(github_sha, 'r') as f:
        return {line.strip() for line in f if line.strip()}

github_headers = {
    'Authorization': "token {}".format(github_token)
}


def checkEnvData():
    if not github_token:
        logging.error("github_token è·å–å¤±è´¥")
        exit(0)
    elif not msg_push.tg_token:  
        logging.error("TG_tokenè·å–å¤±è´¥")
        exit(0)
    elif not msg_push.wechat_token:  
        logging.error("wechat_tokenè·å–å¤±è´¥")
        exit(0)
    elif not msg_push.google_sheet_token:
        logging.error("google_sheet_tokenè·å–å¤±è´¥")
        exit(0)
    elif not msg_push.tg_chat_id:
        logging.error("tg_chat_idè·å–å¤±è´¥")
        exit(0)
    else:
        logging.info("ç¯å¢ƒå˜é‡åŠ è½½æˆåŠŸ")


def init():
    logging.basicConfig(level=logging.INFO)
    logging.info("init application")
    checkEnvData()
    logging.info("start send test msg")
    return

def getRSSNews():
    rss_config = utils.load.json_data_load("./RSSs/rss_config.json")
    for key, config in rss_config.items():
        url = config.get("url")
        file_name = config.get("file")
        if url and file_name:
            parse_rss_feed(url,file_name)

def extract_cve_ids(text):
    """ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰CVEç¼–å·"""
    cve_pattern = r'CVE-\d{4}-\d{4,7}'
    return re.findall(cve_pattern, text, re.IGNORECASE)

def check_cve_in_poc_history(cve_id):
    """æ£€æŸ¥CVEæ˜¯å¦åœ¨å†å²PoCè®°å½•ä¸­ï¼Œè¿”å›PoCé“¾æ¥åˆ—è¡¨"""
    try:
        table_content = msg_push.get_google_sheet("CVE")
        if not table_content or len(table_content) < 2:
            return []
        # è¡¨å¤´ï¼šæ—¶é—´ã€å…³é”®è¯ã€é¡¹ç›®åç§°ã€é¡¹ç›®åœ°å€ã€é¡¹ç›®æè¿°
        headers = table_content[0]
        keyword_idx = headers.index("å…³é”®è¯") if "å…³é”®è¯" in headers else 1
        url_idx = headers.index("é¡¹ç›®åœ°å€") if "é¡¹ç›®åœ°å€" in headers else 3
        
        poc_links = []
        for row in table_content[1:]:
            if len(row) > max(keyword_idx, url_idx):
                keyword = row[keyword_idx].upper() if row[keyword_idx] else ""
                if cve_id.upper() in keyword:
                    poc_links.append(row[url_idx])
        return poc_links
    except Exception as e:
        logging.error(f"æŸ¥è¯¢CVEå†å²PoCå¤±è´¥: {e}")
        return []

def parse_rss_feed(feed_url,file):
    # è§£æRSS feed
    try:
        response = requests.get(feed_url,timeout=20)
    except requests.exceptions.SSLError as ssl_err:
        logging.error(f"SSL é”™è¯¯ï¼šæ— æ³•è¿æ¥ {feed_url}ï¼Œè·³è¿‡è¯¥æ¡ç›®ã€‚é”™è¯¯ä¿¡æ¯ï¼š{ssl_err}")
        return  # å‘ç”Ÿ SSL é”™è¯¯æ—¶è·³è¿‡å½“å‰å¾ªç¯
    except requests.exceptions.RequestException as req_err:
        logging.error(f"è¯·æ±‚é”™è¯¯ï¼š{feed_url}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{req_err}")
        return  # å‘ç”Ÿè¯·æ±‚é”™è¯¯æ—¶è·³è¿‡å½“å‰å¾ªç¯
    except Exception as e:
        logging.error(f"æœªçŸ¥é”™è¯¯ï¼š{feed_url}ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{e}")
        return  # å‘ç”Ÿå…¶ä»–ç±»å‹çš„é”™è¯¯æ—¶è·³è¿‡
    response.encoding = 'utf-8'
    feed_content = response.text
    # è§£æRSS feedå†…å®¹
    feed = feedparser.parse(feed_content)
    if feed.bozo == 1:
        logging.info(file)
        logging.info("è§£æRSS feedæ—¶å‘ç”Ÿé”™è¯¯:", feed.bozo_exception)
        return
    all_entries = utils.load.json_data_load(f"./RSSs/{file}")
    existing_titles = {entry['link'] for entry in all_entries}
    # å®šä¹‰ä¸€ä¸ªæ ‡å¿—ï¼Œæ ‡è®°æ˜¯å¦è¾“å‡ºäº†æ–°å¢æ¡ç›®
    new_entries_found = False
    for entry in feed.entries:
        if entry.link not in existing_titles:
            # è¾“å‡ºæ–°å¢æ¡ç›®
            new_entries_found = True
            all_content_have_cve = True
            logging.info(f"æ ‡é¢˜: {entry.title}  é“¾æ¥: {entry.link}")
            logging.info("-" * 40)
            if file == "google.json":
                if 'cve' not in str(entry.content).lower():
                    all_content_have_cve = False  # å¦‚æœå‘ç°æŸä¸ª content æ²¡æœ‰ "CVE"ï¼Œæ ‡è®°ä¸º False
                    break 
            if file == "vulncheck.json" or file == "securityonline.json" or file == "picus.json" or file == "rapid7.json" or file == "thehackersnews.json":
                if "cve" not in entry.title.lower() or "vulnerabili" not in entry.title.lower():
                    all_content_have_cve = False  # å¦‚æœå‘ç°æŸä¸ª content æ²¡æœ‰ "CVE"ï¼Œæ ‡è®°ä¸º False
            if file == "paloalto.json":
                if "medium" in entry.title.lower() or "low" in entry.title.lower():
                    all_content_have_cve = False  # å¦‚æœå‘ç°æŸä¸ª content æ²¡æœ‰ "CVE"ï¼Œæ ‡è®°ä¸º False
            if file == "gbhackers.json":
                categories = []
                if 'tags' in entry:
                    categories = [tag.term for tag in entry.tags]
                elif 'category' in entry:
                    categories = entry.category if isinstance(entry.category, list) else [entry.category]
                if not any(cat in ["Vulnerability", "Vulnerabilities"] for cat in categories):
                    all_content_have_cve = False   
            all_entries.append({
                    'title': entry.title,
                    'link': entry.link,
                    'published': entry.published
                })
            # å°†æ–°å¢æ¡ç›®æ·»åŠ åˆ°æ–°æ¡ç›®åˆ—è¡¨
            if all_content_have_cve:
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å†å²PoC
                poc_prefix = ""
                cve_ids = extract_cve_ids(entry.title)
                for cve_id in cve_ids:
                    poc_links = check_cve_in_poc_history(cve_id)
                    if poc_links:
                        poc_prefix = f"è¯¥æ¼æ´ç–‘ä¼¼å­˜åœ¨pocæ‰¹é‡ï¼šã€Œ{poc_links[0]}ã€\r\r"
                        logging.info(f"å‘ç°å†å²PoC: {cve_id} -> {poc_links[0]}")
                        break  # æ‰¾åˆ°ä¸€ä¸ªå°±å¤Ÿäº†
                
                msg = f"{poc_prefix}æ ‡é¢˜ï¼š{entry.title}\ré“¾æ¥ï¼š{entry.link}\rå‘å¸ƒæ—¶é—´ï¼š{entry.published}"
                logging.info(f"æ¨é€åˆ°google sheetï¼š{entry.title}  "+entry.link)
                msg_push.wechat_push(msg)
                msg_push.send_google_sheet_githubVul("Emergency Vulnerability","RSS",entry.title,"",entry.link,"")
    # å¦‚æœæœ‰æ–°å¢æ¡ç›®ï¼Œåˆ™æ›´æ–°æ–‡ä»¶
    if new_entries_found:
        utils.load.json_data_save(f"./RSSs/{file}",all_entries)
    else:
        logging.info(f"{file}æœªæ›´æ–°æ–°æ¼æ´")

def get_github_raw_links(github_url):
    # è§£æåœ°å€ï¼Œæå– owner å’Œ repo
    parts = github_url.strip('/').split('/')
    owner, repo = parts[-2], parts[-1]
    
    api_url = f"https://api.github.com/repos/{owner}/{repo}/contents/"
    raw_links = []
    
    try:
        response = requests.get(api_url, headers=github_headers)
        if response.status_code != 200:
            logging.error(f"æå–Rawåœ°å€è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")

            return "å“åº”ç é”™è¯¯"  # è¯·æ±‚å¤±è´¥æˆ–æ— æƒé™
        
        for item in response.json():
            if item['type'] == 'file' and item['name'].endswith(('.py', '.yaml', '.yml')):
                raw_links.append(item['download_url'])
        
        if isinstance(raw_links, list):
            return '\n'.join(raw_links) if raw_links else "æ— è„šæœ¬æ–‡ä»¶"
        return str(raw_links)
    except Exception as e:
        logging.error(f"æå–Rawåœ°å€è¯·æ±‚å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{e}")
        return "ç½‘ç»œé”™è¯¯å¼‚å¸¸"  # ç½‘ç»œé”™è¯¯æˆ–å…¶ä»–å¼‚å¸¸
   
def getKeywordNews(keyword):
    cleanKeywords=set(CleanKeywords)
    today_keyword_info_tmp=[]
    try:
        # æŠ“å–æœ¬å¹´çš„
        #keyword = quote(keyword)
        logging.info(keyword)
        api = "https://api.github.com/search/repositories?q={}&sort=updated".format(keyword)
        json_str = requests.get(api, headers=github_headers, timeout=10).json()
        today_date = datetime.date.today()
        n=20 if len(json_str['items'])>20 else len(json_str['items'])
        for i in range(0, n):
            keyword_url = json_str['items'][i]['html_url']
            try:
                keyword_name = json_str['items'][i]['name']
                description = json_str['items'][i]['description']
                pushed_at_tmp = json_str['items'][i]['pushed_at']
                pushed_at = re.findall('\d{4}-\d{2}-\d{2}', pushed_at_tmp)[0]
                if pushed_at == str(today_date) and keyword_name not in cleanKeywords:
                    msg_push.send_google_sheet("CVE",keyword,keyword_name,keyword_url,description)
                    if "CVE" in keyword:
                        raw_links = get_github_raw_links(keyword_url)
                        msg_push.send_google_raw("raw",keyword_url,raw_links)
                    today_keyword_info_tmp.append({"keyword_name": keyword_name, "keyword_url": keyword_url, "pushed_at": pushed_at,"description":description})

                    logging.info("[+] keyword: {} \n é¡¹ç›®åç§°ï¼š{} \né¡¹ç›®åœ°å€ï¼š{}\næ¨é€æ—¶é—´ï¼š{}\næè¿°ï¼š{}".format(keyword, keyword_name,keyword_url,pushed_at,description))
                else:
                    logging.info("[-] keyword: {} ,{}çš„æ›´æ–°æ—¶é—´ä¸º{}, ä¸å±äºä»Šå¤©".format(keyword, keyword_name, pushed_at))
            except Exception as e:
                pass
    except Exception as e:
        logging.error("Error occurred: %s, githubé“¾æ¥ä¸é€š", e) 
    return today_keyword_info_tmp
    
def getCVE_PoCs():
    #é€šè¿‡å…³é”®è¯æ£€ç´¢PoC
    clean_add = []
    pushdata=list()
    for keyword in keywords:
        templist=getKeywordNews(keyword)
        for tempdata in templist:
            pushdata.append(tempdata)
            clean_add.append(tempdata.get("keyword_name"))
    msg_push.keyword_msg(pushdata)
    if clean_add:
        utils.load.flash_clean_list(clean_add)

def getCISANews():
    with open('./utils/CISA.txt', 'r') as file:
        txt_content = file.read().splitlines()
    url = 'https://www.cisa.gov/sites/default/files/csv/known_exploited_vulnerabilities.csv'
    # è¯»å– CSV å†…å®¹
    try:
        response = requests.get(url)
    except Exception as e:
        # æ•è·å…¶ä»–å¯èƒ½çš„å¼‚å¸¸
        logging.info(f"An unexpected error occurred: {e}")
        return
    response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
    data = response.text  # è·å– CSV æ–‡ä»¶å†…å®¹
    reader = csv.DictReader(data.splitlines())
    msg = ""
    new_cve_list = []
    for row in reader:
        cve = row['cveID']  # å‡è®¾ 'cveID' æ˜¯ CSV ä¸­çš„åˆ—å
        if cve not in txt_content:
            name = row['vulnerabilityName'] + "(" + cve + ")"
            name_cn = utils.load.baidu_api(name)
            shortDescription = row['shortDescription']
            knownRansomwareCampaignUse = row['knownRansomwareCampaignUse']
            shortDescription_cn = utils.load.baidu_api(shortDescription)
            notes = row['notes']
            info = f"åç§°ï¼š{name_cn}\r\næè¿°ï¼š{shortDescription_cn}\r\næ˜¯å¦è¢«å‹’ç´¢åˆ©ç”¨ï¼š{knownRansomwareCampaignUse}\r\né“¾æ¥ï¼š{notes}"
            if not msg:
                msg = "ç¾å›½ç½‘ç»œå®‰å…¨å±€æ¼æ´æ¨é€ï¼š\r\n" + info
            else:
                msg += "\r\n\r\n" + info
            new_cve_list.append(cve)
    
    if new_cve_list:
        logging.info("ä¼å¾®æ¨é€CISAæ¼æ´æ›´æ–°ï¼š"  + ", ".join(new_cve_list))
        msg_push.wechat_push(msg)
        msg_push.tg_push(msg)
        with open("./utils/CISA.txt", 'a') as file:
            for cve in new_cve_list:
                file.write(f"{cve}\n")
    else:
        logging.info("CISAæœªæ›´æ–°æ¼æ´")

def save_file_locally(url, filename):
    try:
        response = requests.get(url,headers=github_headers)
    except Exception as e:
        logging.info(f"An unexpected error occurred: {e}")
    if response.status_code == 200:
        data = response.json()
        aliases = data.get('aliases', [])
        aliases_str = ', '.join(aliases)
        details = data.get('details', '')
        severity = data.get('database_specific', '').get('severity', '')
        for item in known_object:
            if item in details.lower() and severity in ["HIGH","CRITICAL","Unknown","MODERATE"]:
                if item == "jenkins":
                    if "plugin" in details.lower() and "core" not in details.lower():
                        break
                url = f"https://github.com/advisories/{data.get('id', '')}"
                detail = utils.load.baidu_api(details)
                msg = f"ç¼–å·ï¼š{aliases_str}\r\nç»„ä»¶ï¼š{item}\r\nä¿¡æ¯ï¼š{detail}\r\né“¾æ¥ï¼š{url}"
                logging.info(f"ä¼å¾®æ¨é€ï¼š{aliases_str}  "+url)
                msg_push.wechat_push(msg)
                msg_push.send_google_sheet_githubVul("Emergency Vulnerability","github",item,aliases_str,url,detail)
                msg_push.tg_push(msg)
                break
    else:
        logging.info(f"Failed to read {filename}: {response.status_code}")


def getGithubVun():
    url = f"https://api.github.com/repos/github/advisory-database/commits"
    try:
        response = requests.get(url,headers=github_headers)
    except Exception as e:
        logging.error(f"An unexpected error occurred: {e}") 
    if response.status_code == 200:
        latest_commit = response.json()[0]
        commit_message = latest_commit['commit']['message']
        commit_url = latest_commit['html_url']
        commit_sha = latest_commit['sha']
        if not os.path.exists(github_sha):
            open(github_sha, 'w').close()
        with open(github_sha, 'r') as file:
            lines = file.readlines()
            # å¦‚æœaåœ¨æ–‡ä»¶ä¸­ï¼Œåˆ™ç»“æŸå‡½æ•°
            if str(commit_sha) + '\n' in lines:
                logging.info("æ²¡æœ‰æ–°çš„commitè¢«æäº¤")
                return
        with open(github_sha, 'a') as file:
            file.write(str(commit_sha) + '\n')
        logging.info(f"Latest commit message: {commit_message}")
        logging.info(f"Commit URL: {commit_url}")
        # è·å–è¯¦ç»†ä¿®æ”¹å†…å®¹
        commit_details_url = f"https://api.github.com/repos/github/advisory-database/commits/{commit_sha}"
        details_response = requests.get(commit_details_url,headers=github_headers)
        if details_response.status_code == 200:
            commit_details = details_response.json()
            files_changed = commit_details.get('files', [])
            #logging.info("\nFiles changed:")
            for file in files_changed:
                filename = file['filename']
                #additions = file['additions']
                #deletions = file['deletions']
                #changes = file['changes']
                status = file['status']
                if filename.endswith('.json') and status == "added":
                    # æ„å»ºåŸå§‹æ–‡ä»¶çš„ URL
                    raw_url = f"https://raw.githubusercontent.com/github/advisory-database/{commit_sha}/{filename}"
                    save_file_locally(raw_url, filename)
                    logging.info(f"- {filename}: {status} ")
        else:
            logging.info(f"Failed to retrieve commit details: {details_response.status_code}")
            
# è·å–æœ€è¿‘ä¸€æ¬¡æäº¤çš„å˜æ›´æ–‡ä»¶
def get_latest_commit_files(repo,branch):
    try:
        processed_shas = load_processed_shas()
        # åˆ†é¡µè·å–æ–°æäº¤
        page = 1
        max_pages = 2  # æ–°å¢ï¼šæœ€å¤šè·å–2é¡µ
        per_page = 100
        new_shas = []
        # è·å–æœ€æ–°æäº¤ï¼ˆæŒ‡å®šåˆ†æ”¯ï¼‰
        while page <= max_pages:  # ä¿®æ”¹å¾ªç¯æ¡ä»¶
            commits_url = f"https://api.github.com/repos/{repo}/commits?per_page={per_page}&sha={branch}&page={page}"
            response = requests.get(commits_url, headers=github_headers, timeout=10)
            try:
                response.raise_for_status()
            except requests.HTTPError as e:
                logging.error(f"è·å–æäº¤åˆ—è¡¨å¤±è´¥: URL={commits_url}, é”™è¯¯: {str(e)}")
                return []
            commits = response.json()
            if not commits:
                break
            for commit in commits:
                sha = commit["sha"]
                if sha in processed_shas:
                    # é‡åˆ°å·²å¤„ç† SHAï¼Œåœæ­¢éå†
                    break
                new_shas.append(sha)
            # å¦‚æœå½“å‰é¡µæœ‰å·²å¤„ç† SHAï¼Œåœæ­¢ç¿»é¡µ
            if any(sha in processed_shas for sha in new_shas):
                break
            page += 1
        if not new_shas:
            logging.info(f"{repo} æ— æ–°æäº¤")
            return []
        # æ”¶é›†æ‰€æœ‰å˜æ›´æ–‡ä»¶
        all_files = []
        for sha in new_shas:
            details_url = f"https://api.github.com/repos/{repo}/commits/{sha}"
            try:
                details_response = requests.get(details_url, headers=github_headers, timeout=15)
                details_response.raise_for_status()
                commit_data = details_response.json()
            except requests.RequestException as e:
                logging.error(f"è§£æå¤±è´¥: {details_url} - {str(e)}")
                continue
            files = [file["filename"] 
                     for file in commit_data.get("files", [])
                     if file.get("status") == "added" ] # ä»…ä¿ç•™æ–°å¢æ–‡ä»¶]
            all_files.extend(files)
        # æ‰¹é‡è®°å½• SHA
        with open(github_sha, 'a') as f:
            for sha in new_shas:
                f.write(f"{sha}\n")
        logging.info(f"{repo} æœ€æ–°æäº¤ SHA: {new_shas}")
        return all_files
    except requests.RequestException as e:
        logging.error(f"è·å– {repo} æœ€æ–°æäº¤å¤±è´¥: {e}")
        return []

def read_file(repo, branch, file_path):
    url = f"https://raw.githubusercontent.com/{repo}/{branch}/{file_path}"

    try:
        response = requests.get(url, headers=github_headers, timeout=10)
        response.raise_for_status()
        if "/wp-content/plugins/" in response.text and "readme.txt" in response.text:
            logging.info(f"âŒ {file_path}ä¸ºç‰ˆæœ¬å¯¹æ¯”æ’ä»¶")
            return
        if "wp-content/themes" in response.text and "style.css" in response.text:
            logging.info(f"âŒ {file_path}ä¸ºç‰ˆæœ¬å¯¹æ¯”æ’ä»¶")
            return
        msg_push.tg_push(f"{repo}é¡¹ç›®æ–°å¢PoCæ¨é€:\r\nåç§°ï¼š{file_path}\r\nåœ°å€ï¼š{url}")
        msg_push.send_google_sheet("CVE",repo,file_path,url,"")
        logging.info(f"âœ… è·å–æ–‡ä»¶å†…å®¹æˆåŠŸ: {file_path} ")
    except requests.RequestException as e:
        logging.error(f"âŒ è·å–æ–‡ä»¶å†…å®¹å¤±è´¥: {file_path} -> {e}")

def getRepoPoCs():
    for repo in repo_list:
        repo_name = repo["name"]
        folder = repo["folder"].rstrip('/') + '/'  # è§„èŒƒç›®å½•æ ¼å¼
        branch = repo.get("branch", "main")
        changed_files = get_latest_commit_files(repo_name, branch)
        if changed_files is None:
            logging.error(f"âŒ è·å– {repo_name} çš„å˜æ›´æ–‡ä»¶å¤±è´¥ï¼Œå·²è·³è¿‡")
            continue  # é”™è¯¯å·²è®°å½•ï¼Œè·³è¿‡å¤„ç†
        new_files = list({file for file in changed_files if file.startswith(folder)})
        if new_files:
            logging.info(f"ğŸ“¦ {repo_name} å‘ç° {len(new_files)} ä¸ªæ–°æ–‡ä»¶:")
            for idx, file in enumerate(new_files, 1):
                logging.info(f"  {idx}. {file}")
            for file in new_files:
                read_file(repo_name, branch, file)
        else:
            logging.info(f"âœ… {repo_name} çš„ {folder} ç›®å½•æ— æ–°æ–‡ä»¶å˜æ›´")

def main():
    init()
    #ç´§æ€¥æ¼æ´RSSæ¨é€
    logging.info("----------------------------------------------------------")
    logging.info("----------------------ç´§æ€¥æ¼æ´RSSæ¨é€-----------------------")
    logging.info("----------------------------------------------------------")
    getRSSNews()
    #ç´§æ€¥æ¼æ´CISAæ¨é€
    logging.info("----------------------------------------------------------")
    logging.info("----------------------ç´§æ€¥æ¼æ´CISAæ¨é€----------------------")    
    logging.info("----------------------------------------------------------")
    getCISANews()
    #ç´§æ€¥æ¼æ´Githubæ¨é€
    logging.info("----------------------------------------------------------")
    logging.info("---------------------ç´§æ€¥æ¼æ´Githubæ¨é€---------------------")
    logging.info("----------------------------------------------------------")
    getGithubVun()
    #CVEæŠ«éœ²PoCè·å–
    logging.info("----------------------------------------------------------")
    logging.info("-------------------Github CVEå…¬å¼€POCè·å–-------------------")
    logging.info("----------------------------------------------------------")
    getCVE_PoCs()
    #é‡ç‚¹é¡¹ç›®ç›‘æ§
    logging.info("----------------------------------------------------------")
    logging.info("---------------------Github é‡ç‚¹é¡¹ç›®ç›‘æ§--------------------")
    logging.info("----------------------------------------------------------")
    getRepoPoCs()
    return


if __name__ == '__main__':
    main()
